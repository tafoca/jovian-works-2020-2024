{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrrM3Jg3uPPb"
   },
   "source": [
    "# article-recommender\n",
    "\n",
    "Use the \"Run\" button to execute the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxSpqVmwuRs5"
   },
   "source": [
    "# 1 Section:                                                                 \n",
    "Research Area Subject Area Prediction (Large Scale classification) using shallow Multi-Layer Perceptron (MLP) model\n",
    "\n",
    "# 2 Section:\n",
    "Research Paper Recommendation for reading: using sentence transformer model\n",
    "\n",
    "Research Papers dataset link::\n",
    "https://www.kaggle.com/datasets/spsayakpaul/arxiv-paper-abstracts/data\n",
    "\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qF-V1Z6oux42"
   },
   "source": [
    "# Loading tools and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5205,
     "status": "ok",
     "timestamp": 1720953760274,
     "user": {
      "displayName": "Laurent Cabrel Tabueu Fotso",
      "userId": "03330808069489833311"
     },
     "user_tz": -120
    },
    "id": "77bpuKZVuPPd"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ast import literal_eval\n",
    "# is used for safely evaluating strings containing Python literals or container displays\n",
    "# (e.g., lists, dictionaries) to their corresponding Python objects.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "executionInfo": {
     "elapsed": 5419,
     "status": "ok",
     "timestamp": 1720957134512,
     "user": {
      "displayName": "Laurent Cabrel Tabueu Fotso",
      "userId": "03330808069489833311"
     },
     "user_tz": -120
    },
    "id": "U7-HXjmhvWuP",
    "outputId": "fc1d24f7-4400-481e-ba86-3733137c4932"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"arxiv_data\",\n  \"rows\": 56181,\n  \"fields\": [\n    {\n      \"column\": \"terms\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3402,\n        \"samples\": [\n          \"['cs.LG', 'cs.CL', 'cs.CV', 'stat.ML']\",\n          \"['cs.LG', 'cs.AI', 'cs.CV', 'stat.ME', 'stat.ML']\",\n          \"['stat.ML', 'cs.LG', 'physics.chem-ph', 'physics.data-an']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"titles\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 41105,\n        \"samples\": [\n          \"Semi-supervised Federated Learning for Activity Recognition\",\n          \"SATR-DL: Improving Surgical Skill Assessment and Task Recognition in Robot-assisted Surgery with Deep Neural Networks\",\n          \"A Hybrid Stochastic Policy Gradient Algorithm for Reinforcement Learning\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstracts\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 41115,\n        \"samples\": [\n          \"Counterfactual inference has become a ubiquitous tool in online\\nadvertisement, recommendation systems, medical diagnosis, and econometrics.\\nAccurate modeling of outcome distributions associated with different\\ninterventions -- known as counterfactual distributions -- is crucial for the\\nsuccess of these applications. In this work, we propose to model counterfactual\\ndistributions using a novel Hilbert space representation called counterfactual\\nmean embedding (CME). The CME embeds the associated counterfactual distribution\\ninto a reproducing kernel Hilbert space (RKHS) endowed with a positive definite\\nkernel, which allows us to perform causal inference over the entire landscape\\nof the counterfactual distribution. Based on this representation, we propose a\\ndistributional treatment effect (DTE) that can quantify the causal effect over\\nentire outcome distributions. Our approach is nonparametric as the CME can be\\nestimated under the unconfoundedness assumption from observational data without\\nrequiring any parametric assumption about the underlying distributions. We also\\nestablish a rate of convergence of the proposed estimator which depends on the\\nsmoothness of the conditional mean and the Radon-Nikodym derivative of the\\nunderlying marginal distributions. Furthermore, our framework allows for more\\ncomplex outcomes such as images, sequences, and graphs. Our experimental\\nresults on synthetic data and off-policy evaluation tasks demonstrate the\\nadvantages of the proposed estimator.\",\n          \"Current multi-person localisation and tracking systems have an over reliance\\non the use of appearance models for target re-identification and almost no\\napproaches employ a complete deep learning solution for both objectives. We\\npresent a novel, complete deep learning framework for multi-person localisation\\nand tracking. In this context we first introduce a light weight sequential\\nGenerative Adversarial Network architecture for person localisation, which\\novercomes issues related to occlusions and noisy detections, typically found in\\na multi person environment. In the proposed tracking framework we build upon\\nrecent advances in pedestrian trajectory prediction approaches and propose a\\nnovel data association scheme based on predicted trajectories. This removes the\\nneed for computationally expensive person re-identification systems based on\\nappearance features and generates human like trajectories with minimal\\nfragmentation. The proposed method is evaluated on multiple public benchmarks\\nincluding both static and dynamic cameras and is capable of generating\\noutstanding performance, especially among other recently proposed deep neural\\nnetwork based approaches.\",\n          \"Unifying text detection and text recognition in an end-to-end training\\nfashion has become a new trend for reading text in the wild, as these two tasks\\nare highly relevant and complementary. In this paper, we investigate the\\nproblem of scene text spotting, which aims at simultaneous text detection and\\nrecognition in natural images. An end-to-end trainable neural network named as\\nMask TextSpotter is presented. Different from the previous text spotters that\\nfollow the pipeline consisting of a proposal generation network and a\\nsequence-to-sequence recognition network, Mask TextSpotter enjoys a simple and\\nsmooth end-to-end learning procedure, in which both detection and recognition\\ncan be achieved directly from two-dimensional space via semantic segmentation.\\nFurther, a spatial attention module is proposed to enhance the performance and\\nuniversality. Benefiting from the proposed two-dimensional representation on\\nboth detection and recognition, it easily handles text instances of irregular\\nshapes, for instance, curved text. We evaluate it on four English datasets and\\none multi-language dataset, achieving consistently superior performance over\\nstate-of-the-art methods in both detection and end-to-end text recognition\\ntasks. Moreover, we further investigate the recognition module of our method\\nseparately, which significantly outperforms state-of-the-art methods on both\\nregular and irregular text datasets for scene text recognition.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "arxiv_data"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-5aa77e07-e230-464b-9ca7-94046a59f98e\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>terms</th>\n",
       "      <th>titles</th>\n",
       "      <th>abstracts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>Multi-Level Attention Pooling for Graph Neural...</td>\n",
       "      <td>Graph neural networks (GNNs) have been widely ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['cs.LG', 'cs.AI']</td>\n",
       "      <td>Decision Forests vs. Deep Networks: Conceptual...</td>\n",
       "      <td>Deep networks and decision forests (such as ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['cs.LG', 'cs.CR', 'stat.ML']</td>\n",
       "      <td>Power up! Robust Graph Convolutional Network v...</td>\n",
       "      <td>Graph convolutional networks (GCNs) are powerf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['cs.LG', 'cs.CR']</td>\n",
       "      <td>Releasing Graph Neural Networks with Different...</td>\n",
       "      <td>With the increasing popularity of Graph Neural...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>Recurrence-Aware Long-Term Cognitive Network f...</td>\n",
       "      <td>Machine learning solutions for pattern classif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5aa77e07-e230-464b-9ca7-94046a59f98e')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-5aa77e07-e230-464b-9ca7-94046a59f98e button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-5aa77e07-e230-464b-9ca7-94046a59f98e');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-cae2e474-332f-460c-a8eb-378a4739373a\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cae2e474-332f-460c-a8eb-378a4739373a')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-cae2e474-332f-460c-a8eb-378a4739373a button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                           terms  \\\n",
       "0                      ['cs.LG']   \n",
       "1             ['cs.LG', 'cs.AI']   \n",
       "2  ['cs.LG', 'cs.CR', 'stat.ML']   \n",
       "3             ['cs.LG', 'cs.CR']   \n",
       "4                      ['cs.LG']   \n",
       "\n",
       "                                              titles  \\\n",
       "0  Multi-Level Attention Pooling for Graph Neural...   \n",
       "1  Decision Forests vs. Deep Networks: Conceptual...   \n",
       "2  Power up! Robust Graph Convolutional Network v...   \n",
       "3  Releasing Graph Neural Networks with Different...   \n",
       "4  Recurrence-Aware Long-Term Cognitive Network f...   \n",
       "\n",
       "                                           abstracts  \n",
       "0  Graph neural networks (GNNs) have been widely ...  \n",
       "1  Deep networks and decision forests (such as ra...  \n",
       "2  Graph convolutional networks (GCNs) are powerf...  \n",
       "3  With the increasing popularity of Graph Neural...  \n",
       "4  Machine learning solutions for pattern classif...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "path = \"/content/drive/MyDrive/arxiv_data_210930-054931.csv\"\n",
    "import pandas as pd\n",
    "arxiv_data = pd.read_csv(path)\n",
    "arxiv_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ficNWumJCjaX"
   },
   "source": [
    "# Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1720957180148,
     "user": {
      "displayName": "Laurent Cabrel Tabueu Fotso",
      "userId": "03330808069489833311"
     },
     "user_tz": -120
    },
    "id": "Fk1tNMPHCmhZ",
    "outputId": "b6f03668-a869-406f-8f90-35e7d6d38b0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56181, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 851,
     "status": "ok",
     "timestamp": 1720957196221,
     "user": {
      "displayName": "Laurent Cabrel Tabueu Fotso",
      "userId": "03330808069489833311"
     },
     "user_tz": -120
    },
    "id": "-qbpkFQsCsgU",
    "outputId": "2434fed3-da1d-47c9-d669-0f8d7d6b489c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "terms        0\n",
       "titles       0\n",
       "abstracts    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 510,
     "status": "ok",
     "timestamp": 1720957228760,
     "user": {
      "displayName": "Laurent Cabrel Tabueu Fotso",
      "userId": "03330808069489833311"
     },
     "user_tz": -120
    },
    "id": "GxEN0Sl4C0fo",
    "outputId": "7bf1938b-4e80-4e3c-f7c4-2a063c3a8139"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15054"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1295,
     "status": "ok",
     "timestamp": 1720957259806,
     "user": {
      "displayName": "Laurent Cabrel Tabueu Fotso",
      "userId": "03330808069489833311"
     },
     "user_tz": -120
    },
    "id": "tM3CkvyEC78T",
    "outputId": "daeaa79c-9195-4d73-f33e-155871496cef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels : ['cs.LG' 'cs.AI' 'cs.CR' ... 'D.1.3; G.4; I.2.8; I.2.11; I.5.3; J.3'\n",
      " '68T07, 68T45, 68T10, 68T50, 68U35' 'I.2.0; G.3']\n",
      "lenght : 1177\n"
     ]
    }
   ],
   "source": [
    "# getting unique labels\n",
    "labels_column = arxiv_data['terms'].apply(literal_eval)\n",
    "labels = labels_column.explode().unique()\n",
    "print(\"labels :\",labels)\n",
    "print(\"lenght :\",len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 462,
     "status": "ok",
     "timestamp": 1720957425378,
     "user": {
      "displayName": "Laurent Cabrel Tabueu Fotso",
      "userId": "03330808069489833311"
     },
     "user_tz": -120
    },
    "id": "DOSPgbyZDknM",
    "outputId": "ad989338-2a53-4038-99bb-133ee8e7ab07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 41105 rows in the deduplicated dataset.\n",
      "2503\n",
      "3401\n"
     ]
    }
   ],
   "source": [
    "# remove duplicate entries based on the \"titles\" (terms) column\n",
    "# This filters the DataFrame, keeping only the rows where the titles are not duplicated.\n",
    "arxiv_data = arxiv_data[~arxiv_data['titles'].duplicated()]\n",
    "print(f\"There are {len(arxiv_data)} rows in the deduplicated dataset.\")\n",
    "# There are some terms with occurrence as low as 1.\n",
    "print(sum(arxiv_data['terms'].value_counts()==1))\n",
    "# how many unique terms\n",
    "print(arxiv_data['terms'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 423,
     "status": "ok",
     "timestamp": 1720957884395,
     "user": {
      "displayName": "Laurent Cabrel Tabueu Fotso",
      "userId": "03330808069489833311"
     },
     "user_tz": -120
    },
    "id": "fLX4krnKFRH3",
    "outputId": "15f80135-d8a5-4639-893f-e9c834ec2485"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38602, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering the rare terms. (it keeps only those rows where the \"terms\" value occurs more than once in the original DataFrame.)\n",
    "arxiv_data_filtered = arxiv_data.groupby('terms').filter(lambda x: len(x) > 1)\n",
    "arxiv_data_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 906,
     "status": "ok",
     "timestamp": 1720957933807,
     "user": {
      "displayName": "Laurent Cabrel Tabueu Fotso",
      "userId": "03330808069489833311"
     },
     "user_tz": -120
    },
    "id": "KHHOiDirFf9Z",
    "outputId": "82df5b8f-5101-43ab-b272-78edf7287b29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['cs.LG']), list(['cs.LG', 'cs.AI']),\n",
       "       list(['cs.LG', 'cs.CR', 'stat.ML'])], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It evaluates the given string containing a Python literal or container display (e.g., a list or dictionary) and returns the corresponding Python object.\n",
    "arxiv_data_filtered['terms'] = arxiv_data_filtered['terms'].apply(lambda x: literal_eval(x))\n",
    "arxiv_data_filtered['terms'].values[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ja356fPXFrRq"
   },
   "source": [
    "# train and test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 401,
     "status": "ok",
     "timestamp": 1720957993991,
     "user": {
      "displayName": "Laurent Cabrel Tabueu Fotso",
      "userId": "03330808069489833311"
     },
     "user_tz": -120
    },
    "id": "dsWhcPVbFqWM",
    "outputId": "1a6e7203-dbb6-4fb5-ff19-d887463e2a2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in training set: 34741\n",
      "Number of rows in validation set: 1930\n",
      "Number of rows in test set: 1931\n"
     ]
    }
   ],
   "source": [
    "test_split = 0.1\n",
    "\n",
    "# Initial train and test split.\n",
    "# The stratify parameter ensures that the splitting is done in a way that preserves the same distribution of labels (terms) in both the training and test sets.\n",
    "train_df, test_df = train_test_split(arxiv_data_filtered,test_size=test_split,stratify=arxiv_data_filtered[\"terms\"].values,)\n",
    "\n",
    "# Splitting the test set further into validation\n",
    "# and new test sets.\n",
    "val_df = test_df.sample(frac=0.5)\n",
    "test_df.drop(val_df.index, inplace=True)\n",
    "\n",
    "print(f\"Number of rows in training set: {len(train_df)}\")\n",
    "print(f\"Number of rows in validation set: {len(val_df)}\")\n",
    "print(f\"Number of rows in test set: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2185,
     "status": "ok",
     "timestamp": 1720958099722,
     "user": {
      "displayName": "Laurent Cabrel Tabueu Fotso",
      "userId": "03330808069489833311"
     },
     "user_tz": -120
    },
    "id": "GzHLWGzcGIzl",
    "outputId": "06230cb4-7f18-447e-90fb-a9b55296b4ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "\n",
      "['[UNK]', 'cs.CV', 'cs.LG', 'stat.ML', 'cs.AI', 'eess.IV', 'cs.RO', 'cs.CL', 'cs.NE', 'cs.GR', 'cs.CR', 'math.OC', 'eess.SP', 'cs.SI', 'cs.MM', 'cs.SY', 'cs.IR', 'eess.SY', 'cs.MA', 'cs.HC', 'math.IT', 'cs.IT', 'cs.DC', 'stat.AP', 'cs.CY', 'stat.ME', 'stat.TH', 'math.ST', 'eess.AS', 'cs.SD', 'cs.DS', 'q-bio.QM', 'q-bio.NC', 'stat.CO', 'cs.CG', 'cs.NI', 'cs.GT', 'math.NA', 'cs.SE', 'cs.NA', 'I.2.6', 'physics.chem-ph', 'cs.DB', 'physics.comp-ph', 'cond-mat.dis-nn', 'q-bio.BM', 'math.PR', 'cs.PL', 'cs.LO', '68T45', 'cs.AR', 'physics.data-an', 'quant-ph', 'I.2.10', 'cs.CE', 'cond-mat.stat-mech', 'q-fin.ST', 'I.4.6', '68T05', 'math.DS', 'cs.CC', 'physics.ao-ph', 'physics.soc-ph', 'physics.med-ph', 'cs.PF', 'q-bio.GN', 'econ.EM', 'cs.DM', 'I.4.8', 'astro-ph.IM', 'physics.flu-dyn', 'math.AT', 'hep-ex', 'I.4', '68U10', 'q-fin.TR', 'physics.geo-ph', 'cs.FL', 'I.5.4', 'I.2', 'cond-mat.mtrl-sci', 'I.4.9', '68T10', 'physics.optics', 'I.4; I.5', '68T07', 'q-fin.CP', 'math.AP', 'I.2.6; I.2.8', 'I.2.10; I.4; I.5', '65D19', 'q-bio.PE', 'physics.app-ph', 'nlin.CD', 'math.CO', 'cs.MS', 'I.4.5', 'I.2.6; I.5.1', 'I.2.0; I.2.6', '68U01', '68T01', 'hep-ph', 'cs.SC', 'cs.ET', 'K.3.2', 'I.2.8', '68T30', 'q-fin.GN', 'q-fin.EC', 'q-bio.MN', 'econ.GN', 'I.4.9; I.5.4', 'I.4.0', 'I.2; I.5', 'I.2; I.4; I.5', 'I.2.6; I.2.7', 'I.2.10; I.4.8', '68T99', '68Q32', '68', '62H30', 'q-fin.RM', 'q-fin.PM', 'q-bio.TO', 'q-bio.OT', 'physics.plasm-ph', 'physics.class-ph', 'physics.bio-ph', 'nlin.AO', 'math.SP', 'math.MP', 'math.LO', 'math.FA', 'math-ph', 'cs.DL', 'cond-mat.soft', 'I.5.2', 'I.4.6; I.4.8', 'I.4.4', 'I.4.3', 'I.4.1', 'I.3.7', 'I.2; J.2', 'I.2; I.2.6; I.2.7', 'I.2.7', 'I.2.6; I.5.4', 'I.2.6; I.2.9', 'I.2.6; I.2.7; H.3.1; H.3.3', 'I.2.6; I.2.10', 'I.2.6, I.5.4', 'I.2.1; J.3', 'I.2.10; I.5.1; I.4.8', 'I.2.10; I.4.8; I.5.4', 'I.2.10; I.2.6', 'I.2.1', 'H.3.1; I.2.6; I.2.7', 'H.3.1; H.3.3; I.2.6; I.2.7', 'G.3', 'F.2.2; I.2.7', 'E.5; E.4; E.2; H.1.1; F.1.1; F.1.3', '68Txx', '62H99', '62H35', '60L10, 60L20', '14J60 (Primary) 14F05, 14J26 (Secondary)']\n"
     ]
    }
   ],
   "source": [
    "# creates a TensorFlow RaggedTensor (terms) from the values in the \"terms\" column of the train_df DataFrame. A RaggedTensor is a tensor with non-uniform shapes\n",
    "terms = tf.ragged.constant(train_df['terms'].values)\n",
    "# This line creates a StringLookup layer in TensorFlow. The purpose of this layer is to map strings to integer indices and vice versa. The output_mode=\"multi_hot\" indicates that the layer will output a multi-hot encoded representation of the input strings.\n",
    "lookup = tf.keras.layers.StringLookup(output_mode='multi_hot')\n",
    "# This step adapts the StringLookup layer to the unique values in the \"terms\" column, building the vocabulary.\n",
    "lookup.adapt(terms)\n",
    "# retrieve vocabulary\n",
    "vocab = lookup.get_vocabulary()\n",
    "\n",
    "print(\"Vocabulary:\\n\")\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 419,
     "status": "ok",
     "timestamp": 1720958128342,
     "user": {
      "displayName": "Laurent Cabrel Tabueu Fotso",
      "userId": "03330808069489833311"
     },
     "user_tz": -120
    },
    "id": "bRe3B0EUGOEA",
    "outputId": "99034750-a8eb-4429-8b9b-164a0b948b38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label: ['stat.ML', 'cs.LG']\n",
      "Label-binarized representation: [[0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "sample_label = train_df[\"terms\"].iloc[0]\n",
    "print(f\"Original label: {sample_label}\")\n",
    "\n",
    "label_binarized = lookup([sample_label])\n",
    "print(f\"Label-binarized representation: {label_binarized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "executionInfo": {
     "elapsed": 402,
     "status": "ok",
     "timestamp": 1720958265886,
     "user": {
      "displayName": "Laurent Cabrel Tabueu Fotso",
      "userId": "03330808069489833311"
     },
     "user_tz": -120
    },
    "id": "gmfGk05gGxY2",
    "outputId": "99981196-06fe-4d36-ed1b-ed56a1fd032e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nIn summary, the make_dataset function is designed to create a \\ndataset suitable for training a model. It takes a dataframe as input, \\nassumes it has \"abstracts\" and \"terms\" columns, and creates a dataset of \\nbatches where each batch consists of abstract \\nsequences and their corresponding binarized label sequences. \\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# following lines::\n",
    "# which is used for automatic adjustment of resource usage by TensorFlow's data loading pipeline.\n",
    "\n",
    "#max_seqlen: Maximum sequence length. It indicates the maximum length allowed for sequences.\n",
    "max_seqlen = 150\n",
    "#batch_size: Batch size. It specifies the number of samples to use in each iteration.\n",
    "batch_size = 128\n",
    "#padding_token: A token used for padding sequences.\n",
    "padding_token = \"<pad>\"\n",
    "#auto = tf.data.AUTOTUNE: auto is assigned the value tf.data.AUTOTUNE,\n",
    "auto = tf.data.AUTOTUNE\n",
    "\n",
    "def make_dataset(dataframe, is_train=True):\n",
    "    # creating sequences of labesls\n",
    "    labels = tf.ragged.constant(dataframe[\"terms\"].values)\n",
    "    #This line uses the previously defined lookup layer to convert the ragged tensor of labels into a binarized representation. The resulting label_binarized is a NumPy array.\n",
    "    label_binarized = lookup(labels).numpy()\n",
    "    # creating sequences of text.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dataframe[\"abstracts\"].values, label_binarized))\n",
    "    # shuffling data basis on condition\n",
    "    dataset = dataset.shuffle(batch_size * 10) if is_train else dataset\n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "\"\"\"\n",
    "In summary, the make_dataset function is designed to create a\n",
    "dataset suitable for training a model. It takes a dataframe as input,\n",
    "assumes it has \"abstracts\" and \"terms\" columns, and creates a dataset of\n",
    "batches where each batch consists of abstract\n",
    "sequences and their corresponding binarized label sequences.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 438,
     "status": "ok",
     "timestamp": 1720958348129,
     "user": {
      "displayName": "Laurent Cabrel Tabueu Fotso",
      "userId": "03330808069489833311"
     },
     "user_tz": -120
    },
    "id": "hiX0_TGPHA3n"
   },
   "outputs": [],
   "source": [
    "train_dataset = make_dataset(train_df, is_train=True)\n",
    "validation_dataset = make_dataset(val_df, is_train=False)\n",
    "test_dataset = make_dataset(test_df, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 415,
     "status": "ok",
     "timestamp": 1720958645962,
     "user": {
      "displayName": "Laurent Cabrel Tabueu Fotso",
      "userId": "03330808069489833311"
     },
     "user_tz": -120
    },
    "id": "jipS-Ja2HKUc",
    "outputId": "ab7968df-1489-46cc-d4d1-d0a38bc4304c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract: b'Understanding the implicit bias of training algorithms is of crucial\\nimportance in order to explain the success of overparametrised neural networks.\\nIn this paper, we study the dynamics of stochastic gradient descent over\\ndiagonal linear networks through its continuous time version, namely stochastic\\ngradient flow. We explicitly characterise the solution chosen by the stochastic\\nflow and prove that it always enjoys better generalisation properties than that\\nof gradient flow. Quite surprisingly, we show that the convergence speed of the\\ntraining loss controls the magnitude of the biasing effect: the slower the\\nconvergence, the better the bias. To fully complete our analysis, we provide\\nconvergence guarantees for the dynamics. We also give experimental results\\nwhich support our theoretical claims. Our findings highlight the fact that\\nstructured noise can induce better generalisation and they help explain the\\ngreater performances observed in practice of stochastic gradient descent over\\ngradient descent.'\n",
      " \n",
      "Abstract: b'Self-attention architectures, which are rapidly pushing the frontier in\\nnatural language processing, demonstrate a surprising depth-inefficient\\nbehavior: previous works indicate that increasing the internal representation\\n(network width) is just as useful as increasing the number of self-attention\\nlayers (network depth). We theoretically predict a width-dependent transition\\nbetween depth-efficiency and depth-inefficiency in self-attention. We conduct\\nsystematic empirical ablations on networks of depths 6 to 48 that clearly\\nreveal the theoretically predicted behaviors, and provide explicit quantitative\\nsuggestions regarding the optimal depth-to-width allocation for a given\\nself-attention network size. The race towards beyond 1-Trillion parameter\\nlanguage models renders informed guidelines for increasing self-attention depth\\nand width in tandem an essential ingredient. Our guidelines elucidate the\\ndepth-to-width trade-off in self-attention networks of sizes up to the scale of\\nGPT3 (which we project to be too deep for its size), and beyond, marking an\\nunprecedented width of 30K as optimal for a 1-Trillion parameter network.'\n",
      " \n",
      "Abstract: b'Domain adaptation aims to generalize a model from a source domain to tackle\\ntasks in a related but different target domain. Traditional domain adaptation\\nalgorithms assume that enough labeled data, which are treated as the prior\\nknowledge are available in the source domain. However, these algorithms will be\\ninfeasible when only a few labeled data exist in the source domain, and thus\\nthe performance decreases significantly. To address this challenge, we propose\\na Domain-invariant Graph Learning (DGL) approach for domain adaptation with\\nonly a few labeled source samples. Firstly, DGL introduces the Nystrom method\\nto construct a plastic graph that shares similar geometric property as the\\ntarget domain. And then, DGL flexibly employs the Nystrom approximation error\\nto measure the divergence between plastic graph and source graph to formalize\\nthe distribution mismatch from the geometric perspective. Through minimizing\\nthe approximation error, DGL learns a domain-invariant geometric graph to\\nbridge source and target domains. Finally, we integrate the learned\\ndomain-invariant graph with the semi-supervised learning and further propose an\\nadaptive semi-supervised model to handle the cross-domain problems. The results\\nof extensive experiments on popular datasets verify the superiority of DGL,\\nespecially when only a few labeled source samples are available.'\n",
      " \n",
      "Abstract: b'Previous studies on image classification have mainly focused on the\\nperformance of the networks, not on real-time operation or model compression.\\nWe propose a Gaussian Deep Recurrent visual Attention Model (GDRAM)- a\\nreinforcement learning based lightweight deep neural network for large scale\\nimage classification that outperforms the conventional CNN (Convolutional\\nNeural Network) which uses the entire image as input. Highly inspired by the\\nbiological visual recognition process, our model mimics the stochastic location\\nof the retina with Gaussian distribution. We evaluate the model on Large\\ncluttered MNIST, Large CIFAR-10 and Large CIFAR-100 datasets which are resized\\nto 128 in both width and height.'\n",
      " \n",
      "Abstract: b'We provide a novel convergence analysis of two popular sampling algorithms,\\nWeighted Kernel Herding and Sequential Bayesian Quadrature, that are used to\\napproximate the expectation of a function under a distribution. Existing\\ntheoretical analysis was insufficient to explain the empirical successes of\\nthese algorithms. We improve upon existing convergence rates to show that,\\nunder mild assumptions, these algorithms converge linearly. To this end, we\\nalso suggest a simplifying assumption that is true for most cases in finite\\ndimensions, and that acts as a sufficient condition for linear convergence to\\nhold in the much harder case of infinite dimensions. When this condition is not\\nsatisfied, we provide a weaker convergence guarantee. Our analysis also yields\\na new distributed algorithm for large-scale computation that we prove converges\\nlinearly under the same assumptions. Finally, we provide an empirical\\nevaluation to test the proposed algorithm for a real world application.'\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# This code snippet is iterating through batches of the training dataset and printing the abstract text along with the corresponding labels.\n",
    "text_batch, label_batch = next(iter(train_dataset))\n",
    "for i, text in enumerate(text_batch[:5]):\n",
    "    label = label_batch[i].numpy()[None, ...]\n",
    "    print(f\"Abstract: {text}\")\n",
    "    #print(f\"Label(s): {invert_multi_hot(label[0])}\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2547,
     "status": "ok",
     "timestamp": 1720958658308,
     "user": {
      "displayName": "Laurent Cabrel Tabueu Fotso",
      "userId": "03330808069489833311"
     },
     "user_tz": -120
    },
    "id": "alAKW0XSIQZr",
    "outputId": "273bede8-12ea-483d-cbfb-ded6beca9680"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159122\n"
     ]
    }
   ],
   "source": [
    "# This code calculates the size of the vocabulary in the \"abstracts\" column of the train_df DataFrame.\n",
    "\n",
    "# Creating vocabulary with uniques words\n",
    "vocabulary = set()\n",
    "train_df[\"abstracts\"].str.lower().str.split().apply(vocabulary.update)\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qftuqCR9IV1j"
   },
   "source": [
    "# Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 19722,
     "status": "ok",
     "timestamp": 1720958715179,
     "user": {
      "displayName": "Laurent Cabrel Tabueu Fotso",
      "userId": "03330808069489833311"
     },
     "user_tz": -120
    },
    "id": "k82g3XC-IUpk"
   },
   "outputs": [],
   "source": [
    "# Initializes a TextVectorization layer\n",
    "text_vectorizer = layers.TextVectorization(max_tokens=vocabulary_size,ngrams=2,output_mode=\"tf_idf\")\n",
    "# `TextVectorization` layer needs to be adapted as per the vocabulary from our\n",
    "# training set.\n",
    "text_vectorizer.adapt(train_dataset.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 389,
     "status": "ok",
     "timestamp": 1720958730917,
     "user": {
      "displayName": "Laurent Cabrel Tabueu Fotso",
      "userId": "03330808069489833311"
     },
     "user_tz": -120
    },
    "id": "_s5LyOstIjUt"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mapping Vectorization to Datasets: The code maps the text vectorization operation to\n",
    "each element of the training, validation, and test datasets. This ensures that the text\n",
    "data in each dataset is transformed into numerical vectors using the adapted TextVectorization layer.\n",
    "The num_parallel_calls parameter is used to parallelize the mapping process, and prefetch is\n",
    "applied to prefetch data batches\n",
    "for better performance.\n",
    "\"\"\"\n",
    "train_dataset = train_dataset.map(lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto).prefetch(auto)\n",
    "validation_dataset = validation_dataset.map(lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto).prefetch(auto)\n",
    "test_dataset = test_dataset.map(lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto).prefetch(auto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4AKIO26IyBl"
   },
   "source": [
    "# model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J4cvPeOtIzcF",
    "outputId": "c063e7b4-12cb-4288-f7ac-dfbd163976d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "272/272 [==============================] - 669s 2s/step - loss: 0.0494 - binary_accuracy: 0.9826 - val_loss: 0.0181 - val_binary_accuracy: 0.9946\n",
      "Epoch 2/20\n",
      "272/272 [==============================] - 655s 2s/step - loss: 0.0175 - binary_accuracy: 0.9950 - val_loss: 0.0176 - val_binary_accuracy: 0.9947\n",
      "Epoch 3/20\n",
      "172/272 [=================>............] - ETA: 3:53 - loss: 0.0138 - binary_accuracy: 0.9959"
     ]
    }
   ],
   "source": [
    "# creating shallow_mlp_model  (MLP)\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Creating shallow_mlp_model (MLP) with dropout layers\n",
    "model1 = keras.Sequential([\n",
    "    # First hidden layer: 512 neurons, ReLU activation function, with dropout.\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dropout(0.5),  # Adding dropout for regularization.\n",
    "\n",
    "    # Second hidden layer: 256 neurons, ReLU activation function, with dropout.\n",
    "    layers.Dense(256, activation=\"relu\"),\n",
    "    layers.Dropout(0.5),  # Adding dropout for regularization.\n",
    "\n",
    "    # Output layer: The number of neurons equals the vocabulary size (output vocabulary of the StringLookup layer), with a sigmoid activation function.\n",
    "    layers.Dense(lookup.vocabulary_size(), activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model1.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['binary_accuracy'])\n",
    "\n",
    "# Add early stopping\n",
    "# Number of epochs with no improvement after which training will be stopped.\n",
    "# Restore weights from the epoch with the best value of the monitored quantity.\n",
    "early_stopping = EarlyStopping(patience=5,restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "# Add early stopping callback.verbose=1\n",
    "history = model1.fit(train_dataset,validation_data=validation_dataset,epochs=20,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cFdJA9WIOUc5"
   },
   "outputs": [],
   "source": [
    "# plotting loss\n",
    "def plot_result(item):\n",
    "    plt.plot(history.history[item], label=item)\n",
    "    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(item)\n",
    "    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_result(\"loss\")\n",
    "plot_result(\"binary_accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16ejdsvSObtD"
   },
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GCjv3oHwOdrZ"
   },
   "outputs": [],
   "source": [
    "# model evaltuation on test and val dataset\n",
    "_, binary_acc1 = model1.evaluate(test_dataset)\n",
    "_, binary_acc2 = model1.evaluate(validation_dataset)\n",
    "\n",
    "print(f\"Categorical accuracy on the test set: {round(binary_acc1 * 100, 2)}%.\")\n",
    "print(f\"Categorical accuracy on the validation set: {round(binary_acc2 * 100, 2)}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwW39YRfOhj7"
   },
   "source": [
    "# Save Model and Text Vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QdICW5awOkj4"
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model1.save(\"models/model.h5\")\n",
    "\n",
    "# Save the configuration of the text vectorizer\n",
    "saved_text_vectorizer_config = text_vectorizer.get_config()\n",
    "with open(\"models/text_vectorizer_config.pkl\", \"wb\") as f:\n",
    "    pickle.dump(saved_text_vectorizer_config, f)\n",
    "\n",
    "\n",
    "# Save the vocabulary\n",
    "with open(\"models/vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2XUpTYDOoeo"
   },
   "source": [
    "# Load Model and Text Vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xjzseLyzOnZP"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import pickle\n",
    "\n",
    "# Load the model\n",
    "loaded_model = keras.models.load_model(\"models/model.h5\")\n",
    "\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Load the configuration of the text vectorizer\n",
    "with open(\"models/text_vectorizer_config.pkl\", \"rb\") as f:\n",
    "    saved_text_vectorizer_config = pickle.load(f)\n",
    "\n",
    "# Create a new TextVectorization layer with the saved configuration\n",
    "loaded_text_vectorizer = TextVectorization.from_config(saved_text_vectorizer_config)\n",
    "\n",
    "# Load the saved weights into the new TextVectorization layer\n",
    "with open(\"models/text_vectorizer_weights.pkl\", \"rb\") as f:\n",
    "    weights = pickle.load(f)\n",
    "    loaded_text_vectorizer.set_weights(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1Wxbn1yOu6i"
   },
   "outputs": [],
   "source": [
    "# Load the vocabulary\n",
    "with open(\"models/vocab.pkl\", \"rb\") as f:\n",
    "    loaded_vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1QkYAi6OxZy"
   },
   "source": [
    "# Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0ET2gzYO1NE"
   },
   "outputs": [],
   "source": [
    "def invert_multi_hot(encoded_labels):\n",
    "    \"\"\"Reverse a single multi-hot encoded label to a tuple of vocab terms.\"\"\"\n",
    "    hot_indices = np.argwhere(encoded_labels == 1.0)[..., 0]\n",
    "    return np.take(loaded_vocab, hot_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vFBSRYNO7tK"
   },
   "outputs": [],
   "source": [
    "def predict_category(abstract, model, vectorizer, label_lookup):\n",
    "    # Preprocess the abstract using the loaded text vectorizer\n",
    "    preprocessed_abstract = vectorizer([abstract])\n",
    "\n",
    "    # Make predictions using the loaded model\n",
    "    predictions = model.predict(preprocessed_abstract)\n",
    "\n",
    "    # Convert predictions to human-readable labels\n",
    "    predicted_labels = label_lookup(np.round(predictions).astype(int)[0])\n",
    "\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDRokhukPM8O"
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "new_abstract = \"Graph neural networks (GNNs) have been widely used to learn vector\\nrepresentation of graph-structured data and achieved better task performance\\nthan conventional methods. The foundation of GNNs is the message passing\\nprocedure, which propagates the information in a node to its neighbors. Since\\nthis procedure proceeds one step per layer, the range of the information\\npropagation among nodes is small in the lower layers, and it expands toward the\\nhigher layers. Therefore, a GNN model has to be deep enough to capture global\\nstructural information in a graph. On the other hand, it is known that deep GNN\\nmodels suffer from performance degradation because they lose nodes' local\\ninformation, which would be essential for good model performance, through many\\nmessage passing steps. In this study, we propose multi-level attention pooling\\n(MLAP) for graph-level classification tasks, which can adapt to both local and\\nglobal structural information in a graph. It has an attention pooling layer for\\neach message passing step and computes the final graph representation by\\nunifying the layer-wise graph representations. The MLAP architecture allows\\nmodels to utilize the structural information of graphs with multiple levels of\\nlocalities because it preserves layer-wise information before losing them due\\nto oversmoothing. Results of our experiments show that the MLAP architecture\\nimproves the graph classification performance compared to the baseline\\narchitectures. In addition, analyses on the layer-wise graph representations\\nsuggest that aggregating information from multiple levels of localities indeed\\nhas the potential to improve the discriminability of learned graph\\nrepresentations.\"\n",
    "predicted_categories = predict_category(new_abstract, loaded_model, loaded_text_vectorizer, invert_multi_hot)\n",
    "print(\"Predicted Categories:\", predicted_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aXlElS1uPPuK"
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "new_abstract = 'Deep networks and decision forests (such as random forests and gradient\\nboosted trees) are the leading machine learning methods for structured and\\ntabular data, respectively. Many papers have empirically compared large numbers\\nof classifiers on one or two different domains (e.g., on 100 different tabular\\ndata settings). However, a careful conceptual and empirical comparison of these\\ntwo strategies using the most contemporary best practices has yet to be\\nperformed. Conceptually, we illustrate that both can be profitably viewed as\\n\"partition and vote\" schemes. Specifically, the representation space that they\\nboth learn is a partitioning of feature space into a union of convex polytopes.\\nFor inference, each decides on the basis of votes from the activated nodes.\\nThis formulation allows for a unified basic understanding of the relationship\\nbetween these methods. Empirically, we compare these two strategies on hundreds\\nof tabular data settings, as well as several vision and auditory settings. Our\\nfocus is on datasets with at most 10,000 samples, which represent a large\\nfraction of scientific and biomedical datasets. In general, we found forests to\\nexcel at tabular and structured data (vision and audition) with small sample\\nsizes, whereas deep nets performed better on structured data with larger sample\\nsizes. This suggests that further gains in both scenarios may be realized via\\nfurther combining aspects of forests and networks. We will continue revising\\nthis technical report in the coming months with updated results.'\n",
    "predicted_categories = predict_category(new_abstract, loaded_model, loaded_text_vectorizer, invert_multi_hot)\n",
    "print(\"Predicted Categories:\", predicted_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7TuZfqcPPTC6"
   },
   "outputs": [],
   "source": [
    "# great resutls..................................."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WfHpu9dPWnC"
   },
   "source": [
    "# =======Section 2========"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_4Og9txPYZC"
   },
   "source": [
    "# 2 Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i8aFMj10Pcnq"
   },
   "outputs": [],
   "source": [
    "arxiv_data.drop(columns = [\"terms\",\"abstracts\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TnSDehfjPh5L"
   },
   "outputs": [],
   "source": [
    "arxiv_data.drop_duplicates(inplace= True)\n",
    "arxiv_data.reset_index(drop= True,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "415ynFqhPipI"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "arxiv_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UM9Qkl7vPmJh"
   },
   "source": [
    "# Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "acU-CjJRPqKM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}