{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test IENeetLayer module\n",
    "#This class stores main information about the layer \n",
    "# and allow to modify some layer parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "### Load the necessary libraries\n",
    "import os\n",
    "from openvino.inference_engine import IENetwork, IECore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "CPU_EXTENSION = \"/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_sse4.so\"\n",
    "\n",
    "def get_args():\n",
    "    '''\n",
    "    Gets the arguments from the command line.\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser(\"Load an IR into the Inference Engine\")\n",
    "    # -- Create the descriptions for the commands\n",
    "    m_desc = \"The location of the model XML file\"\n",
    "\n",
    "    # -- Create the arguments\n",
    "    parser.add_argument(\"-m\", help=m_desc)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_IE(model_xml):\n",
    "    ### Load the Inference Engine API\n",
    "    plugin = IECore()\n",
    "\n",
    "    ### Load IR files into their related class\n",
    "    model_bin = os.path.splitext(model_xml)[0] + \".bin\"\n",
    "    net = IENetwork(model=model_xml, weights=model_bin)\n",
    "\n",
    "    ### Add a CPU extension, if applicable.\n",
    "    plugin.add_extension(CPU_EXTENSION, \"CPU\")\n",
    "\n",
    "    ### Get the supported layers of the network\n",
    "    supported_layers = plugin.query_network(network=net, device_name=\"CPU\")\n",
    "\n",
    "    ### Check for any unsupported layers, and let the user\n",
    "    ### know if anything is missing. Exit the program, if so.\n",
    "    unsupported_layers = [l for l in net.layers.keys() if l not in supported_layers]\n",
    "    if len(unsupported_layers) != 0:\n",
    "        print(\"Unsupported layers found: {}\".format(unsupported_layers))\n",
    "        print(\"Check whether extensions are available to add to IECore.\")\n",
    "        exit(1)\n",
    "\n",
    "    ### Load the network into the Inference Engine\n",
    "    plugin.load_network(net, \"CPU\")\n",
    "\n",
    "    print(\"IR successfully loaded into Inference Engine.\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execution\n",
    "#python feed_network.py -m /home/workspace/models/human-pose-estimation-0001.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IR successfully loaded into Inference Engine.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    #args = get_args()\n",
    "    load_to_IE(\"../ex02/models/human-pose-estimation-0001.xml\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
